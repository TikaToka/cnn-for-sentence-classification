{
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "b306fc9a4baaf5515baf43d83914e9872778cec54f9c49a6a8fd06629a9f9072"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#load data\r\n",
    "from datasets import load_dataset\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "dataset = load_dataset(\"nsmc\")\r\n",
    "\r\n",
    "train_df = pd.DataFrame(dataset['train'])\r\n",
    "test_df = pd.DataFrame(dataset['test'])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (C:\\Users\\ford0\\.cache\\huggingface\\datasets\\nsmc\\default\\1.1.0\\bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#simplify\r\n",
    "\r\n",
    "import re\r\n",
    "\r\n",
    "docs = dataset['train']['document'] + dataset['test']['document']\r\n",
    "label = dataset['train']['label'] + dataset['test']['label']\r\n",
    "\r\n",
    "processed_docs = [re.sub(\"[\\s]+\", \" \", re.sub(\"[^가-힣a-zA-Z0-9]\", \" \", doc)) for doc in docs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "processed_docs[20:50]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['나름 심오한 뜻도 있는 듯 그냥 학생이 선생과 놀아나는 영화는 절대 아님',\n",
       " '보면서 웃지 않는 건 불가능하다',\n",
       " '재미없다 지루하고 같은 음식 영화인데도 바베트의 만찬하고 넘 차이남 바베트의 만찬은 이야기도 있고 음식 보는재미도 있는데 이건 볼게없다 음식도 별로 안나오고 핀란드 풍경이라도 구경할랫는데 그것도 별로 안나옴 ',\n",
       " '절대 평범한 영화가 아닌 수작이라는걸 말씀드립니다 ',\n",
       " '주제는 좋은데 중반부터 지루하다',\n",
       " '다 짤랐을꺼야 그래서 납득할 수 없었던거야 그럴꺼야 꼭 그랬던걸꺼야 ',\n",
       " 'kl2g 고추를 털어버려야 할텐데',\n",
       " '카밀라벨 발연기',\n",
       " '재밋는뎅',\n",
       " '센스있는 연출력 탁월한 캐스팅 90년대의 향수 그래서 9점 ',\n",
       " '엄포스의 위력을 다시 한번 깨닫게 해준 적 남 꽃검사님도 연기 정말 좋았어요 완전 명품드라마 ',\n",
       " '졸쓰레기 진부하고말도안됌 아 시간아까워',\n",
       " '재밌는데 별점이 왜이리 낮은고',\n",
       " '1 라도 기대했던 내가 죄인입니다 죄인입니다 ',\n",
       " '아직도 이 드라마는 내인생의 최고 ',\n",
       " '패션에 대한 열정 안나 윈투어 ',\n",
       " '키이라 나이틀리가 연기하고자 했던건 대체 정신장애일까 틱장애일까',\n",
       " '허허 원작가 정신나간 유령이라 재미있겠네요 ',\n",
       " '포스터는 있어보이는데 관객은 114명이네',\n",
       " '이 영화가 왜 이렇게 저평가 받는지 모르겠다',\n",
       " '단순하면서 은은한 매력의 영화',\n",
       " ' 다 알바생인가 내용도 없고 무서운거도 없고 웃긴거도 하나도 없음 완전 별싱거운 영화 내 시간 넘 아까움 완전 낚임',\n",
       " '오게두어라 서리한이 굶주렸다 ',\n",
       " '정말 맘에 들어요 그래서 또 보고싶은데 또 보는 방법이 없네 ',\n",
       " '윤제문이라는 멋진 배우를 발견하게 됐어요 소소한 일탈이 잔잔한 미소를 머금게 합니다 음악은 조금 아쉽네요 8점 주고 싶은데 평점 올리고 싶어 10점 줄게요 ',\n",
       " '평점에속지마시길시간낭비 돈낭비임',\n",
       " '리얼리티가 뛰어나긴 한데 큰 공감은 안간다 이민기캐릭터는 정신의학상 분노조절장애 초기 증상일거다 툭하면 사람패고 욕하고 물건 파손하고 조금 오바였음 극 초반엔 신선했는데 가면 갈수록 이민기 정신상태 공감불가 ',\n",
       " '마이너스는 왜없냐 뮤비 보고 영화수준 딱 알만하더군 북한에서 이런거 만들라고 돈 대주던 ',\n",
       " '난 우리영화를 사랑합니다 ',\n",
       " '데너리스 타르 가르엔 나도 용의주인이 되고 싶다 누이랑 근친상간이나 하고 다닐지라도 소설 속에선 제일 멋진 놈이 자이메 라니스터였는데 드라마속에선 드래곤 용 이 제일 멋지네 웃음 감독님 토르 2 다크 월드는 말아 잡수셨을지라도 기본 선방은 했음']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from konlpy.tag import Okt\r\n",
    "\r\n",
    "def tokenize(docs):\r\n",
    "\r\n",
    "  okt = Okt()\r\n",
    "\r\n",
    "  for doc in docs:\r\n",
    "    tokenlist = okt.pos(doc)\r\n",
    "    temp = []\r\n",
    "    for w in tokenlist:\r\n",
    "      if w[1] in ['Noun', 'Verb', 'Adjective', 'Adverb', 'Exclamation', 'Foreign', 'Alpha', 'Number', 'Unknown']:\r\n",
    "        temp.append(w[0])\r\n",
    "    res.append(temp)\r\n",
    "  return res\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# tokenize\r\n",
    "\r\n",
    "# from konlpy.tag import Mecab\r\n",
    "\r\n",
    "# mecab = Mecab()\r\n",
    "\r\n",
    "# res = []\r\n",
    "\r\n",
    "# for doc in docs:\r\n",
    "#   tokenlist = mecab.pos(doc)\r\n",
    "#   for w in tokenlist:\r\n",
    "#     if w[1] in ['NNG', 'NNP', 'NNB', 'NNBC', 'NP' 'VV', 'VA', 'IC', 'SN', 'SL', 'MAG', 'MAJ']:\r\n",
    "#       res.append(w[1])\r\n",
    "\r\n",
    "from konlpy.tag import Okt\r\n",
    "import pickle\r\n",
    "\r\n",
    "okt = Okt()\r\n",
    "\r\n",
    "res = []\r\n",
    "\r\n",
    "if not os.path.isfile('senspos.pickle'):\r\n",
    "  for doc in docs:\r\n",
    "    tokenlist = okt.pos(doc)\r\n",
    "    temp = []\r\n",
    "    for w in tokenlist:\r\n",
    "      if w[1] in ['Noun', 'Verb', 'Adjective', 'Adverb', 'Exclamation', 'Foreign', 'Alpha', 'Number', 'Unknown']:\r\n",
    "        temp.append(w[0])\r\n",
    "    res.append(temp)\r\n",
    "\r\n",
    "  with open(\"senspos.pickle\", 'wb') as f:\r\n",
    "    pickle.dump(res, f)\r\n",
    "  \r\n",
    "else:\r\n",
    "  with open(\"senspos.pickle\", 'rb') as f:\r\n",
    "    res = pickle.load(f)\r\n",
    "  \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# padding\r\n",
    "maxlen = max(len(x) for x in res)\r\n",
    "padded_sens = []\r\n",
    "for i in range(len(res)):\r\n",
    "  sen = res[i]\r\n",
    "  temp = sen + [\" <PAD/>\"] * (maxlen - len(sen))\r\n",
    "  padded_sens.append(temp)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# vocab to index\r\n",
    "\r\n",
    "import nltk\r\n",
    "tokens = [t for d in padded_sens for t in d]\r\n",
    "text = nltk.Text(tokens, name='NSMC')\r\n",
    "word_count = text.vocab()\r\n",
    "vocabulary_inv = [x[0] for x in word_count.most_common()]\r\n",
    "vocabulary = {x:i for i, x in enumerate(vocabulary_inv)}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\r\n",
    "\r\n",
    "vocabulary_inv[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<PAD/>', '영화', '너무', '정말', '진짜', '이', '점', '연기', '평점', '것']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocabulary.items()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# word 2 vec\r\n",
    "from gensim.models import word2vec\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "w2v = word2vec.Word2Vec.load(\"../data/w2v/ko.bin\")\r\n",
    "\r\n",
    "w2v = {w: w2v[w] if w in w2v else np.random.uniform(-0.25, 0.25, w2v.vector_size) for w in vocabulary_inv}\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ford0\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "C:\\Users\\ford0\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "w2v['안녕']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-4.48758937e-02, -5.26038110e-01,  2.18058324e+00, -2.70098805e-01,\n",
       "       -1.22612631e+00, -4.35174644e-01,  8.78038049e-01,  1.78903091e+00,\n",
       "        5.13344407e-01,  8.00866067e-01,  1.33781850e-01,  4.23676372e-01,\n",
       "        6.25718057e-01, -1.49838462e-01, -2.65716583e-01, -1.18363452e+00,\n",
       "        1.09828494e-01,  6.56335890e-01,  9.90437388e-01, -2.88908720e-01,\n",
       "       -4.82253104e-01, -1.71825096e-01, -5.22048175e-01, -1.37081638e-01,\n",
       "        4.01715070e-01, -3.54384005e-01, -4.38563734e-01,  3.37939188e-02,\n",
       "       -4.21380460e-01, -2.11324722e-01,  5.34786046e-01,  5.26668072e-01,\n",
       "       -3.78847629e-01, -1.59311071e-01,  1.77993524e+00, -1.28354877e-01,\n",
       "        4.53990996e-01,  6.38647795e-01,  3.22042465e-01, -6.49676397e-02,\n",
       "       -9.30086493e-01,  6.84082747e-01, -1.41875729e-01, -1.49376774e+00,\n",
       "        1.44657120e-03,  4.33810472e-01, -7.31874406e-02,  7.40463585e-02,\n",
       "       -6.44519210e-01,  1.11461449e+00, -8.20367783e-02, -5.76601565e-01,\n",
       "        1.71026981e+00,  5.43356836e-01, -1.38096079e-01, -7.13480115e-01,\n",
       "        1.37917042e-01,  1.31351754e-01, -1.64170414e-01,  5.08485079e-01,\n",
       "        1.03430068e+00, -5.08647382e-01,  3.78100970e-03,  4.74166781e-01,\n",
       "        5.89607954e-01,  3.45575511e-01,  8.35864961e-01,  8.41770291e-01,\n",
       "        9.89145637e-01, -2.74089158e-01,  1.25443026e-01,  1.02799308e+00,\n",
       "       -4.82123911e-01, -1.69151232e-01, -2.85922945e-01, -8.49736392e-01,\n",
       "        8.46186206e-02, -1.81830883e+00, -5.13923943e-01, -2.16713548e-01,\n",
       "       -1.31658185e+00, -1.02243495e+00,  8.11321557e-01,  2.19687596e-02,\n",
       "       -6.71579957e-01, -5.98710179e-01, -7.66289771e-01, -1.32737505e+00,\n",
       "       -6.29475042e-02, -1.27141750e+00, -9.70623434e-01, -2.50643134e-01,\n",
       "        5.53779125e-01, -9.36333358e-01, -3.81229192e-01,  9.78302062e-02,\n",
       "       -1.73168528e+00,  2.34338660e-02,  5.90916038e-01,  1.10269034e+00,\n",
       "       -1.77305982e-01,  5.86390793e-01, -2.25778148e-01,  1.85514867e-01,\n",
       "       -8.05007041e-01,  5.64193904e-01, -9.69711125e-01, -3.67523193e-01,\n",
       "       -1.23882771e+00, -4.01369512e-01, -1.19125068e-01, -4.65394631e-02,\n",
       "       -2.21846491e-01,  1.82357505e-02,  4.56880897e-01, -2.14156687e-01,\n",
       "       -3.06043118e-01, -1.71571866e-01,  6.13820255e-01, -4.26191688e-01,\n",
       "        5.79584062e-01,  1.03633344e-01,  6.82440162e-01, -1.95503905e-01,\n",
       "        5.96677661e-01,  6.77304804e-01, -1.25491381e+00,  3.14139165e-02,\n",
       "        1.33252859e+00, -1.52929530e-01, -5.01865447e-01,  4.62395728e-01,\n",
       "        6.49050236e-01,  4.93880659e-01,  1.26435041e-01,  3.00690591e-01,\n",
       "        3.55630815e-01, -2.70101190e-01,  5.62555134e-01, -1.03386605e+00,\n",
       "       -3.14555407e-01,  3.89042318e-01,  1.03528607e+00, -4.63004947e-01,\n",
       "       -6.14613831e-01, -2.57924795e-01, -1.05875695e+00,  1.89552337e-01,\n",
       "        5.63951731e-01,  1.79895550e-01,  2.87300587e-01, -1.12558770e+00,\n",
       "       -1.56612837e+00,  6.86075389e-01,  6.99334323e-01, -3.55547547e-01,\n",
       "        6.66430950e-01, -2.01317817e-01,  9.82120037e-01,  1.48326457e-02,\n",
       "        5.74257970e-01,  5.32084644e-01, -5.38365960e-01,  1.24933779e-01,\n",
       "       -3.11034471e-01,  5.36548980e-02,  4.54503685e-01,  1.85251966e-01,\n",
       "        1.31985113e-01,  2.39911228e-01, -9.13004637e-01, -1.16619909e+00,\n",
       "        8.83497417e-01,  3.04435015e-01, -7.70304739e-01, -7.39239752e-02,\n",
       "       -1.10028563e-02,  5.21843016e-01, -9.24460530e-01,  5.88730514e-01,\n",
       "        6.41907156e-02, -5.43511808e-01, -2.87139893e-01,  2.77004838e-01,\n",
       "       -4.67248596e-02, -5.69744051e-01, -6.73322231e-02,  3.29014838e-01,\n",
       "       -6.28687918e-01, -5.23731291e-01,  6.91283345e-01, -1.03363819e-01,\n",
       "       -7.24156022e-01, -5.24323702e-01, -5.47199488e-01,  2.67527951e-03,\n",
       "        5.74104823e-02, -1.35755897e+00,  4.52745229e-01, -3.91733170e-01],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_dim = 200\r\n",
    "filter_sizes = (3, 4, 5)\r\n",
    "num_filters = 100\r\n",
    "dropout = 0.5\r\n",
    "hidden_dims = 100\r\n",
    "\r\n",
    "batch_size = 50\r\n",
    "num_epochs = 10\r\n",
    "min_word_count = 1\r\n",
    "context = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# X_train = tokenize(train_df['document'])\r\n",
    "X_train = res[:len(train_df)]\r\n",
    "y_train = train_df['label']\r\n",
    "# X_test = tokenize(test_df['document'])\r\n",
    "X_test = res[len(train_df):]\r\n",
    "y_test = test_df['label']\r\n",
    "\r\n",
    "X_train = np.stack([np.stack([w2v[w] for w in sen]) for sen in X_train])\r\n",
    "X_test = np.stack([np.stack([w2v[w] for w in sen]) for sen in X_test])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "X_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow import keras\r\n",
    "\r\n",
    "m = keras.layers.Input(shape=(X_test.shape[1] ,embedding_dim))\r\n",
    "\r\n",
    "# build conv block\r\n",
    "conv_blocks = []\r\n",
    "for s in filter_sizes:\r\n",
    "  conv = keras.layers.Conv1D(filters=num_filters,\r\n",
    "                         kernel_size=s,\r\n",
    "                         padding=\"valid\",\r\n",
    "                         activation=\"relu\",\r\n",
    "                         strides=1)(z)\r\n",
    "  conv = keras.layers.MaxPooling1D(pool_size=2)(conv)\r\n",
    "  conv = keras.layers.Flatten()(conv)\r\n",
    "  conv_blocks.append(conv)\r\n",
    "m = keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\r\n",
    "\r\n",
    "m = keras.layers.Dropout(dropout)(m)\r\n",
    "m = keras.layers.Dense(hidden_dims, activation=\"relu\")(m)\r\n",
    "model_output = keras.layers.Dense(1, activation=\"sigmoid\")(m)\r\n",
    "\r\n",
    "model = keras.Model(model_input, model_output)\r\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])\r\n",
    "\r\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\r\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}