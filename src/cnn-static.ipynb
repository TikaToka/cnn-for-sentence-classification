{
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "b306fc9a4baaf5515baf43d83914e9872778cec54f9c49a6a8fd06629a9f9072"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#load data\r\n",
    "from datasets import load_dataset\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "dataset = load_dataset(\"nsmc\")\r\n",
    "\r\n",
    "train_df = pd.DataFrame(dataset['train'])\r\n",
    "test_df = pd.DataFrame(dataset['test'])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (C:\\Users\\ford0\\.cache\\huggingface\\datasets\\nsmc\\default\\1.1.0\\bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#simplify\r\n",
    "\r\n",
    "import re\r\n",
    "\r\n",
    "docs = dataset['train']['document'] + dataset['test']['document']\r\n",
    "label = dataset['train']['label'] + dataset['test']['label']\r\n",
    "\r\n",
    "processed_docs = [re.sub(\"[\\s]+\", \" \", re.sub(\"[^가-힣a-zA-Z0-9]\", \" \", doc)) for doc in docs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# tokenize\r\n",
    "\r\n",
    "# from konlpy.tag import Mecab\r\n",
    "\r\n",
    "# mecab = Mecab()\r\n",
    "\r\n",
    "# res = []\r\n",
    "\r\n",
    "# for doc in docs:\r\n",
    "#   tokenlist = mecab.pos(doc)\r\n",
    "#   for w in tokenlist:\r\n",
    "#     if w[1] in ['NNG', 'NNP', 'NNB', 'NNBC', 'NP' 'VV', 'VA', 'IC', 'SN', 'SL', 'MAG', 'MAJ']:\r\n",
    "#       res.append(w[1])\r\n",
    "\r\n",
    "from konlpy.tag import Okt\r\n",
    "import pickle\r\n",
    "\r\n",
    "okt = Okt()\r\n",
    "\r\n",
    "res = []\r\n",
    "\r\n",
    "if not os.path.isfile('senspos.pickle'):\r\n",
    "  for doc in docs:\r\n",
    "    tokenlist = okt.pos(doc)\r\n",
    "    temp = []\r\n",
    "    for w in tokenlist:\r\n",
    "      if w[1] in ['Noun', 'Verb', 'Adjective', 'Adverb', 'Exclamation', 'Foreign', 'Alpha', 'Number', 'Unknown']:\r\n",
    "        temp.append(w[0])\r\n",
    "    res.append(temp)\r\n",
    "\r\n",
    "  with open(\"senspos.pickle\", 'wb') as f:\r\n",
    "    pickle.dump(res, f)\r\n",
    "  \r\n",
    "else:\r\n",
    "  with open(\"senspos.pickle\", 'rb') as f:\r\n",
    "    res = pickle.load(f)\r\n",
    "  \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# padding\r\n",
    "maxlen = max(len(x) for x in res)\r\n",
    "padded_sens = []\r\n",
    "for i in range(len(res)):\r\n",
    "  sen = res[i]\r\n",
    "  temp = sen + [\" <PAD/>\"] * (maxlen - len(sen))\r\n",
    "  padded_sens.append(temp)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# vocab to index\r\n",
    "\r\n",
    "import nltk\r\n",
    "tokens = [t for d in padded_sens for t in d]\r\n",
    "text = nltk.Text(tokens, name='NSMC')\r\n",
    "word_count = text.vocab()\r\n",
    "vocabulary_inv = [x[0] for x in word_count.most_common()]\r\n",
    "vocabulary = {x:i for i, x in enumerate(vocabulary_inv)}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# padded sens to index sens\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "x = np.array([[vocabulary[w] for w in sentence] for sentence in padded_sens])\r\n",
    "y = np.array(label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# word 2 vec\r\n",
    "from gensim.models import word2vec\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "w2v = word2vec.Word2Vec.load(\"../data/w2v/ko.bin\")\r\n",
    "\r\n",
    "w2v = {k: w2v[w] if w in w2v else np.random.uniform(-0.25, 0.25, w2v.vector_size) for w, k in vocabulary.items()}\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ford0\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "C:\\Users\\ford0\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# args setting\r\n",
    "\r\n",
    "embedding_dim = 200\r\n",
    "filter_sizes = (3, 4, 5)\r\n",
    "num_filters = 100\r\n",
    "dropout = 0.5\r\n",
    "hidden_dims = 100\r\n",
    "\r\n",
    "batch_size = 32\r\n",
    "num_epochs = 10\r\n",
    "min_word_count = 1\r\n",
    "context = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# make dataset to use at training\r\n",
    "\r\n",
    "try:\r\n",
    "  X_train = np.load('xtrain.npy')\r\n",
    "  y_train = y[:len(train_df)]\r\n",
    "  X_test = np.load('xtest.npy')\r\n",
    "  y_test = y[len(train_df):]\r\n",
    "  \r\n",
    "except:\r\n",
    "\r\n",
    "  X_train = x[:len(train_df)]\r\n",
    "  y_train = y[:len(train_df)]\r\n",
    "  X_test = x[len(train_df):]\r\n",
    "  y_test = y[len(train_df):]\r\n",
    "\r\n",
    "  X_train = np.stack([np.stack([w2v[w] for w in sen]) for sen in X_train])\r\n",
    "  X_test = np.stack([np.stack([w2v[w] for w in sen]) for sen in X_test])\r\n",
    "\r\n",
    "  np.save('xtrain', X_train)\r\n",
    "  np.save('xtest', X_test)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "  np.save('xtrain', X_train)\r\n",
    "  np.save('xtest', X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# setting model and train\r\n",
    "\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "inp = keras.layers.Input(shape=(X_test.shape[1] ,embedding_dim))\r\n",
    "\r\n",
    "m = inp\r\n",
    "m = keras.layers.Dropout(dropout)(m)\r\n",
    "\r\n",
    "# build conv block\r\n",
    "conv_blocks = []\r\n",
    "for s in filter_sizes:\r\n",
    "  conv = keras.layers.Conv1D(filters=num_filters,\r\n",
    "                         kernel_size=s,\r\n",
    "                         padding=\"valid\",\r\n",
    "                         activation=\"relu\",\r\n",
    "                         strides=1)(m)\r\n",
    "  conv = keras.layers.MaxPooling1D(pool_size=2)(conv)\r\n",
    "  conv = keras.layers.Flatten()(conv)\r\n",
    "  conv_blocks.append(conv)\r\n",
    "m = keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\r\n",
    "\r\n",
    "m = keras.layers.Dropout(dropout)(m)\r\n",
    "m = keras.layers.Dense(hidden_dims, activation=\"relu\")(m)\r\n",
    "out = keras.layers.Dense(1, activation=\"sigmoid\")(m)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.Model(inp, out)\r\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n",
    "\r\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\r\n",
    "          validation_data=(X_test, y_test), verbose=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/conv1d/conv1d (defined at <ipython-input-12-6f7f836e2dc1>:5) ]] [Op:__inference_train_function_1158]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6f7f836e2dc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs,\n\u001b[1;32m----> 5\u001b[1;33m           validation_data=(X_test, y_test), verbose=2)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/conv1d/conv1d (defined at <ipython-input-12-6f7f836e2dc1>:5) ]] [Op:__inference_train_function_1158]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}